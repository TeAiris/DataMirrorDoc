{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Intro/Concept","text":""},{"location":"#introconcept","title":"Intro/Concept","text":""},{"location":"#data-mirror-fragmented-projections-of-oneself","title":"Data Mirror: Fragmented Projections of Oneself","text":"<p>Data Mirror is an interactive spatial-audio installation that transforms collective presence into evolving sound and image, showing how shared spaces, and the people within them, continuously reshape one another.  </p>"},{"location":"#concept-overview","title":"Concept Overview","text":"<p>Data Mirror is an immersive spatial-audio and visual installation that places participants inside a reactive matrix-like environment driven by movement, proximity, and sonic interaction. A multistem musical composition plays through a spatial audio array, while visuals projected across the front wall reveal the system\u2019s internal \u201cdata world\u201d, a 3D space containing matrix-inspired effects, a stylized humanoid avatar, and a virtual representation of the speaker array.  </p> <p> </p> <p>Concentration = max(0.0, 1.0 \u2212 min(\u03c3x / 0.5, 1.0))</p> <p>Participants can manipulate sound using a Nulea M512 Wireless Trackball Mouse, which controls real-time spatialization within Unity. They can cycle through several stem combinations (drums, bass, harmony, melody) and control how those stems are spatially mirrored through different panning patterns. Stem loops come from eight pre-built musical sets created in Ableton, and these sets progress throughout the installation.  </p> <p>An Intel RealSense depth camera analyzes crowd movement and estimates a real-time center of mass. This position data modulates the global tempo. As users group together or spread apart, the music subtly shifts, reinforcing the idea that the audience, the environment, and the composition form a shared, responsive ecosystem.  </p>"},{"location":"#motivation-and-inspiration","title":"Motivation and Inspiration","text":"<p> Visual inspiration from The Matrix universe.</p> <p> Dolby Atmos, spatial panning, or object-based audio in Dolby Atmos Renderer.</p> <p>Our motivation for creating Data Mirror grew from several intersecting influences that shaped both the aesthetics and the technical design of the installation. We were inspired by The Matrix, particularly its visual language and its exploration of digital embodiment, perception, and identity. We were also motivated by Dolby Atmos, especially its object-based spatial panning and mirrored-panning techniques, which shaped how we approached sound as a movable, expressive entity within space.  </p> <p>Alongside these inspirations, our earlier work with spatial panning via OSC naturally evolved into a unified concept. These combined ideas shaped Data Mirror into an environment that reflects how people interact with digital systems and how their presence continuously transforms sound and image in real time.  </p>"},{"location":"#goals-and-learning-outcomes","title":"Goals and Learning Outcomes","text":""},{"location":"#audience-goals","title":"Audience Goals","text":"<ul> <li>Feel empowered and challenged by controlling spatial audio that responds to their actions.  </li> <li>Understand how collective movement alters the sonic and visual environment, even when one person believes they are in control.  </li> <li>Experience sound as a malleable and reactive substance rather than a fixed playback.  </li> </ul>"},{"location":"#creator-learning-goals","title":"Creator Learning Goals","text":"<ul> <li>Develop a portable and repeatable workflow for multichannel spatial installations.  </li> <li>Gain a deeper understanding of IEM spatialization, OSC routing, and Unity to DAW communication.  </li> <li>Prototype interaction models that may extend into future VR and MR spatial-music research.  </li> </ul>"},{"location":"#role-of-interactivity-and-media","title":"Role of Interactivity and Media","text":"<p>Interactivity is central to the meaning of Data Mirror: - Sound: Stems move based on user input and spatial mapping, creating a strong sense of agency. - Visuals: Matrix-style shaders, reactive colored orbs, and a humanoid data avatar reflect participant influence. - Shared presence: Crowd movement shapes global musical parameters, demonstrating how individual and collective behaviors influence the system.  </p>"},{"location":"bpdistribution/","title":"BPDistribution","text":""},{"location":"bpdistribution/#body-position-distribution-overview","title":"Body Position Distribution Overview","text":"<p>The Python script implements a real-time computer vision system to track people in a video feed and analyze their horizontal distribution. It then sends this distribution data as Open Sound Control (OSC) messages to Max for Live for sound control.</p> <p></p>"},{"location":"bpdistribution/#overall-purpose","title":"Overall Purpose","text":"<p>The script takes video input from a specified camera, uses background subtraction to isolate moving objects (assumed to be people), tracks these objects over time, and calculates the following metrics for the detected crowd:</p> <ol> <li>Count: The number of stable, currently tracked people.  </li> <li>Average X: The horizontal center-of-mass of the crowd, normalized from 0.0 (far left) to 1.0 (far right).  </li> <li>Concentration: A measure of how tightly clustered the people are horizontally, ranging from 0.0 (maximally spread) to 1.0 (maximally clustered).</li> </ol>"},{"location":"bpdistribution/#dependencies-and-setup","title":"Dependencies and Setup","text":"<ul> <li>cv2 (OpenCV): video capture and computer vision processing.  </li> <li>pythonosc.udp_client: send OSC messages over the network.  </li> <li>numpy: numerical operations.</li> </ul>"},{"location":"bpdistribution/#main-tracking-and-osc-logic","title":"Main Tracking and OSC Logic","text":""},{"location":"bpdistribution/#initialization","title":"Initialization","text":"<ol> <li>OSC Client: udp_Client.SimpleUDPClient is initialized to send messages to the specified IP and port.  </li> <li>Video Capture: cv2.VideoCapture  </li> <li>Background Subtraction: cv2.createBackgroundSubtractorMOG2 learns the stationary background and outputs a mask showing only moving foreground objects.  </li> <li>Tracking State: tracks is used to store the state of currently tracked people (position, size, last time seen, number of hits).</li> </ol>"},{"location":"bpdistribution/#frame-processing-computer-vision","title":"Frame Processing (Computer Vision)","text":"<ol> <li>Noise Reduction: A Gaussian blur is applied to the frame before background subtraction.  </li> <li>Morphological Operations: The foreground mask is cleaned using Opening (removes small noise) and Closing (fills small holes).  </li> <li>Dilation: The mask is slightly dilated to merge adjacent, broken-up blobs into single detections.  </li> <li>Thresholding: A final binary threshold is applied to ensure the mask consists only of black and white pixels.  </li> <li>Contour Finding: cv2.findContours finds distinct shapes in the mask. Those smaller than args.min_area are discarded.</li> </ol>"},{"location":"bpdistribution/#tracking","title":"Tracking","text":"<p>The script implements a simple tracking algorithm based on the horizontal center of each detection.</p> <ol> <li>Matching: Each new detection is compared to existing active tracks. It finds the nearest track based on the x-coordinate distance. A match is confirmed if the distance is less than 1/6 of the frame width (w / match_dist_fraction).  </li> <li>Updating: If a match is found, the track\u2019s position is updated, and its last_seen time and hits count are incremented.  </li> <li>New Tracks: If a detection cannot be matched to a nearby existing track, a new track ID is assigned.  </li> <li>Aging Out: Tracks that have not been seen (detected) for longer than args.hold seconds are removed from the system.</li> </ol>"},{"location":"bpdistribution/#osc-message-sending","title":"OSC Message Sending","text":"<p>OSC messages are sent at the rate defined by args.fps.</p> <ol> <li> <p>Active Tracks Filtering: Only tracks that have been seen recently (last_seen within args.hold) AND have been stable (hits &gt;= args.persist) are considered active tracks for distribution analysis.</p> </li> <li> <p>Calculations:  </p> </li> <li>Normalized X: The horizontal center (cx) of each active track is normalized to the frame width (w) to get a value between 0.0 and 1.0.  </li> <li>Average X (\u03bcx): Calculated as the mean of all normalized X-positions.  </li> <li>Concentration: Calculated from the standard deviation (\u03c3x) of the normalized X-positions:  </li> </ol> <p></p> <ul> <li>A low \u03c3x (people clustered) results in a high concentration score (close to 1.0).  </li> <li> <p>A high \u03c3x (people spread out) results in a low concentration score (close to 0.0).</p> </li> <li> <p>Sending Messages: The following three OSC messages are sent:</p> </li> <li> <p><code>/people/count</code>: The number of active people  </p> </li> <li><code>/people/avgx</code>: The average normalized X-position (center of crowd)  </li> <li><code>/people/concentration</code>: The calculated concentration value</li> </ul>"},{"location":"bpdistribution/#visualization","title":"Visualization","text":"<p>If the \u2014show argument is used, a debug window is displayed showing:</p> <ul> <li>Green rectangles around the current frame\u2019s detections.  </li> <li>Circles marking the center of each track.  </li> <li>Blue circles indicate stable tracks (hits &gt;= persist).  </li> <li>Red circles indicate new/unstable tracks.  </li> <li>The opacity of the track circle fades as its last_seen time approaches the args.hold limit.</li> </ul>"},{"location":"bpdistribution/#cleanup","title":"Cleanup","text":"<p>When the script exits (by pressing \u2018q' in the display window), the camera is released, and all OpenCV windows are closed.</p>"},{"location":"final-installation/","title":"Final Installation","text":""},{"location":"final-installation/#final-installation","title":"Final Installation","text":""},{"location":"final-installation/#location","title":"Location","text":"<p>Underground Atlanta</p>"},{"location":"final-installation/#demo-video","title":"Demo Video","text":""},{"location":"final-installation/#installation-setup","title":"Installation Setup","text":""},{"location":"final-installation/#physical-layout","title":"Physical Layout","text":"<ul> <li>Speaker Array: </li> <li>QSC K10.2 units arranged in a perimeter arc (floor level)  </li> <li>Two elevated Neumann KH120 at the front for height cues  </li> <li>Neumann KH120 used for clarity and spatial detail  (head level)</li> <li> <p>Alto Professional TS18C Subwoofer placed behind the curtain</p> </li> <li> <p>Control Station: </p> </li> <li>MacBook Pro 14\" running Unity + OSC  </li> <li>Nulea M512 Wireless Trackball Mouse  </li> <li> <p>Midas M32 serving as mixer and audio interface  </p> </li> <li> <p>Visual System: </p> </li> <li>Projector displaying Unity\u2019s matrix world + reactive avatar  </li> <li>Intel RealSense camera mounted  </li> <li>Roland V-1HD for blending camera + Unity output  </li> </ul> <p></p>"},{"location":"final-installation/#user-journey","title":"User Journey","text":""},{"location":"final-installation/#arrival","title":"Arrival","text":"<p>Visitors walk into a dim room illuminated by the matrix projection and a pulsing humanoid figure.</p> <p></p>"},{"location":"final-installation/#onboarding","title":"Onboarding","text":"<p>A facilitator briefly explains: - How to use the trackball - How movement affects the system - What to expect sonically and visually  </p>"},{"location":"final-installation/#interaction","title":"Interaction","text":"<ul> <li>Users spatialize stems using the Nulea M512 trackball  </li> <li>Other user's movement affects tempo and energy</li> <li>The installation cycles through stem sets across the evening  </li> </ul>"},{"location":"final-installation/#accessibility-inclusivity","title":"Accessibility &amp; Inclusivity","text":"<ul> <li>Open floor plan with no required reach constraints  </li> <li>Trackball mouse usable while seated or standing  </li> </ul>"},{"location":"final-installation/#media","title":"Media","text":""},{"location":"final-installation/#reflection","title":"Reflection","text":""},{"location":"final-installation/#what-worked-well","title":"What Worked Well","text":"<ul> <li>Strong coherence between visuals, sound, and interaction  </li> <li>Trackball panning felt intuitive and engaging  </li> <li>Spatial system adapted well to the unique architecture of Underground Atlanta  </li> </ul>"},{"location":"final-installation/#challenges","title":"Challenges","text":"<ul> <li>OSC Communication with Python (due to network)  </li> <li>Ableton using Keyserver shuts down periodically (room router had no Internet access)</li> </ul>"},{"location":"final-installation/#future-improvements","title":"Future Improvements","text":"<ul> <li>Expand sound set  </li> <li>Include different forms of visualization  </li> <li>More interactive elements for users  </li> </ul>"},{"location":"tech-details/","title":"Tech Details","text":""},{"location":"tech-details/#tech-details","title":"Tech Details","text":""},{"location":"tech-details/#hardware","title":"Hardware","text":"<ul> <li>MacBook Pro 14\" (Apple Silicon), main control computer running Ableton Live 12, Unity + OSC routing to Reaper  </li> <li>Midas M32, used as both the audio interface and multichannel mixer  </li> <li>Neumann KH 120, primary spatial speakers (head-level arc and front overhead)  </li> <li>QSC K10.2, floor-level reinforcement  </li> <li>Alto Professional TS18C Subwoofer, low-end reinforcement  </li> <li>Projector, front-wall visuals (model varies) </li> <li>Roland V-1HD Video Mixer, blends camera feed and OpenCV output with Unity projections  </li> <li>Intel RealSense D455f Depth Camera, crowd tracking and centroid estimation  </li> <li>Nulea M512 Wireless Trackball Mouse, spatial-panning controller  </li> </ul>"},{"location":"tech-details/#software","title":"Software","text":"<ul> <li>Unity, visual environment, shader system, OSC output  </li> <li>Ableton Live, 8-scene stem engine  </li> <li>Reaper, IEM spatial audio processing  </li> </ul>"},{"location":"tech-details/#software-icons","title":"Software Icons","text":""},{"location":"tech-details/#libraries-plugins","title":"Libraries / Plugins","text":"<ul> <li>IEM Ambisonics Suite (Object-Based Panner, Scene Rotator, In-Ear, etc.)  </li> <li>#extOSC \u2013 Open Sound Control, system-wide OSC routing  </li> <li>OpenCV, depth tracking and centroid extraction  </li> </ul>"},{"location":"tech-details/#libraries-graphic","title":"Libraries Graphic","text":""},{"location":"tech-details/#real-time-matrix-vfx","title":"Real-Time Matrix VFX","text":"<p>MatrixVFX</p> <p> </p>"},{"location":"tech-details/#system-diagram-data-flow","title":"System Diagram / Data Flow","text":""},{"location":"tech-details/#interaction-design","title":"Interaction Design","text":""},{"location":"tech-details/#user-actions","title":"User Actions","text":"<ul> <li>Move through physical space  </li> <li>Use the Nulea trackball to spatialize stems  </li> <li>Toggle stem groups and mirror modes  </li> <li>Observe visuals reacting to movement and input  </li> </ul>"},{"location":"tech-details/#system-responses","title":"System Responses","text":"<ul> <li>Real-time spatial changes  </li> <li>Matrix shader effects and avatar feedback  </li> <li>Tempo changes based on group clustering  </li> <li>Progression through eight stem sets  </li> </ul>"},{"location":"tech-details/#implementation-details","title":"Implementation Details","text":""},{"location":"tech-details/#unity","title":"Unity","text":"<ul> <li>Matrix shader environment  </li> <li>3D speaker-array visualization  </li> <li>Data-avatar representation  </li> <li>See Unity tab. </li> </ul>"},{"location":"tech-details/#reaper","title":"Reaper","text":"<ul> <li>Four routed tracks: Drums, Bass, Harmony, Melody </li> <li>IEM suite for spatialization  </li> <li>OSC parameter mapping  </li> </ul>"},{"location":"tech-details/#ableton","title":"Ableton","text":"<ul> <li>Eight musical scenes  </li> <li>Outputs four stems directly to Reaper  </li> <li>Receives tempo modulation from Unity  </li> </ul>"},{"location":"tech-details/#bpdistributionspy","title":"BPDistributions.py","text":"<ul> <li>see BPDistrubtion tab</li> </ul>"},{"location":"tech-details/#requirements-setup","title":"Requirements &amp; Setup","text":""},{"location":"tech-details/#software-needed","title":"Software Needed","text":"<ul> <li>Unity  </li> <li>Reaper + IEM plugins  </li> <li>Ableton Live  </li> <li>Python 3  </li> </ul>"},{"location":"tech-details/#hardware-connections","title":"Hardware Connections","text":"<ul> <li>M32 \u2192 all speakers  </li> <li>Projector \u2192 HDMI  </li> <li>RealSense camera \u2192 USB-C  </li> <li>Trackball \u2192 USB Dongle or Bluetooth</li> </ul>"},{"location":"tech-details/#running-the-system","title":"Running the System","text":"<ol> <li>Launch Reaper  </li> <li>Load Ableton stem sets  </li> <li>Open Unity scene  </li> <li>Confirm OSC routing  </li> <li>Begin interaction  </li> </ol>"},{"location":"unity/","title":"Unity","text":""},{"location":"unity/#unity-overview","title":"Unity Overview","text":"<p>Unity served as the primary interface for programming interaction, spatial audio control, and responsive visuals throughout the project. It functioned as the central hub bridging visual effects, OSC communication, and interactive 3D elements.</p> <p></p>"},{"location":"unity/#matrix-visual-effects-shaders","title":"Matrix Visual Effects &amp; Shaders","text":"<p>The Matrix VFX library and shader tools were used to stylize the virtual space with falling green characters. These shaders were extended to support:</p> <ul> <li>HSV color manipulation  </li> <li>Adjustable speed and density  </li> <li>Scalable flow patterns  </li> <li>Mapping the Matrix effect directly onto a 3D avatar mesh  </li> </ul> <p>This pipeline allowed the Matrix aesthetic to function as both a background effect and an integrated visual element on characters and world geometry.</p> <p></p>"},{"location":"unity/#avatar-integration-animation","title":"Avatar Integration &amp; Animation","text":"<p>Subtle idle animations were added to the avatar to create natural, life-like movement. When combined with shader-driven Matrix text mapped to the avatar, the visual system became more expressive and immersive.</p> <p></p>"},{"location":"unity/#virtual-reconstruction-of-physical-space","title":"Virtual Reconstruction of Physical Space","text":"<p>To align virtual and real-world systems, the installation environment at Underground Atlanta was modeled in detail.</p> <ul> <li>DWG files from Global Truss were used for dimensional accuracy  </li> <li>The spatial speaker array was modeled and positioned to match the physical setup  </li> <li>The completed structure was exported as FBX and imported into Unity  </li> </ul> <p>Once inside Unity, Matrix character effects were applied to the walls, and a grid-texture contrast layer was added to create motion depth. Additional camera distortion effects enhanced immersion and drew participants deeper into the environment.</p> <p></p>"},{"location":"unity/#spatial-audio-interaction-system","title":"Spatial Audio Interaction System","text":""},{"location":"unity/#spatial-animation-player","title":"Spatial Animation Player","text":"<p>The Spatial Animation Player script is the core interaction system. It enables intuitive visualization and control of spatial audio.</p> <p>Key components include:</p> <ul> <li>A defined 3D interactive zone  </li> <li>Glowing orb markers for sound origins  </li> <li>Mirrored spatial layouts (inspired by Dolby Atmos panning tools)  </li> <li>OSC routing via extOSC to the IEM Stereo Encoder inside Reaper  </li> </ul> <p>Unity\u2019s world-space coordinates are continuously converted into:</p> <ul> <li>Azimuth </li> <li>Elevation </li> <li>Roll </li> </ul> <p>These values drive Reaper\u2019s spatial panner for each stem track.</p> <p>With Unity and Reaper connected at runtime, the Nulea M512 trackball mouse provides fluid, intuitive control of sound object positioning.</p> <p></p>"},{"location":"unity/#spatialmouse-extensions","title":"SpatialMouse Extensions","text":"<p>The SpatialMouse script expands interactivity and visualization through real-time mouse-based effects:</p> <ul> <li>Light-based highlights projected onto virtual speakers  </li> <li>Visual cues that help users understand sound direction  </li> <li>A cycling system for toggling stem-mirroring modes via the top-left mouse button  </li> </ul> <p>These additions create a more expressive and approachable spatial mixing interface, blending sound control with visual feedback.</p> <p></p>"}]}